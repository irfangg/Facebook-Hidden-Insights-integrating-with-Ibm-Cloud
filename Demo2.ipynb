{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License."
        }, 
        {
            "source": "# Analyze Facebook Data Using IBM Watson and IBM Data Platform\n\nThis is a three-part notebook written in `Python_3.5` meant to show how anyone can enrich and analyze a combined dataset of unstructured and structured information with IBM Watson and IBM Data Platform. For this example we are using a standard Facebook Analytics export which features texts from posts, articles and thumbnails, along with standard performance metrics such as likes, shares, and impressions. \n\n**Part I** will use the Natual Language Understanding, Visual Recognition and Tone Analyzer Services from IBM Watson to enrich the Facebook Posts, Thumbnails, and Articles by pulling out `Emotion Tones`, `Social Tones`, `Language Tones`, `Entities`, `Keywords`, and `Document Sentiment`. The end result of Part I will be additional features and metrics we can test, analyze, and visualize in Part III. \n\n**Part II** will be used to set up the visualizations and tests we will run in Part III. The end result of Part II will be multiple pandas DataFrames that will contain the values, and metrics needed to find insights from the Part III tests and experiments.\n\n**Part III** will include services from IBM's Data Platform, including IBM's own data visualization library PixieDust. In Part III we will run analysis on the data from the Facebook Analytics export, such as the number of likes, comments, shares, to the overall reach for each post, and will compare it to the enriched data we pulled in Part I.\n\n\n#### You should only need to change data in the Setup portion of this notebook. All places where you see  <span style=\"color: red\"> User Input </span> is where you should be adding inputs. \n\n### Table of Contents\n\n### [**Part I - Enrich**](#part1)<br>\n1. [Setup](#setup)<br>\n   1.1 [Install Watson Developer Cloud and BeautifulSoup Packages](#setup1)<br>\n   1.2 [Install PixieDust](#pixie)<br> \n   1.3 [Restart Kernel](#restart)<br> \n   1.4 [Import Packages and Libraries](#setup2)<br>\n   1.5 [Add Service Credentials From IBM Cloud for Watson Services](#setup3)<br>\n2. [Load Data](#load)<br>\n   2.1 [Load Data From SoftLayer's Object Storage as a Pandas DataFrame](#load1)<br>\n3. [Prepare Data](#prepare)<br>\n   3.1 [Data Cleansing with Python](#prepare1)<br>\n   3.2 [Beautiful Soup to Extract Thumbnails and Extented Links](#prepare2)<br>\n4. [Enrich Data](#enrich)<br>\n   4.1 [NLU for Post Text](#nlupost)<br>\n   4.2 [NLU for Thumbnail Text](#nlutn)<br>\n   4.3 [NLU for Article Text](#nlulink)<br>\n   4.4 [Tone Analyzer for Post Text](#tonepost)<br>\n   4.5 [Tone Analyzer for Article Text](#tonearticle)<br>\n   4.6 [Visual Recognition](#visual)<br>\n5. [Write Data](#write)<br>\n   5.1 [Convert DataFrame to new CSV](#write1)<br>\n   5.2 [Write Data to SoftLayer's Object Storage](#write2)<br>\n    \n### [**Part II - Data Preparation**](#part2)<br>\n1. [Prepare Data](#prepare)<br>\n   1.1 [Create Multiple DataFrames for Visualizations](#visualizations)<br>\n   1.2 [Create a Consolidated Tone DataFrame](#tone)<br>\n   1.3 [Create a Consolidated Keyword DataFrame](#keyword)<br>\n   1.4 [Create a Consolidated Entity DataFrame](#entity)<br>\n  \n### [**Part III - Analyze**](#part3)<br>\n\n1. [Setup](#2setup)<br> \n    1.1 [Assign Variables](#2setup2)<br>\n2. [Visualize Data](#2visual)<br>\n    2.1 [Run PixieDust Visualization Library with Display() API](#2visual1)\n   \n### Learn more about the technology used:\n\n* [Natual Language Understanding](https://www.ibm.com/watson/developercloud/natural-language-understanding.html)\n* [Tone Analyzer](https://www.ibm.com/watson/developercloud/tone-analyzer.html)\n* [Visual Recognition](https://www.ibm.com/watson/services/visual-recognition/)\n* [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n* [PixieDust](https://github.com/ibm-cds-labs/pixiedust) (Part III)", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### Sample Documents\n[Sample Facebook Posts](https://github.com/IBM/pixiedust-facebook-analysis/blob/master/data/example_input/example_facebook_data.csv) - This is a sample export of IBM Watson's Facebook Page. Engagement metrics such as clicks, impressions, etc. are all changed and do not reflect any actual post performance data.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"part1\"></a>\n#  Part I - Enrich\n\n<a id=\"setup\"></a>\n## 1. Setup\n<a id=\"setup1\"></a>\n### 1.1 Install Latest Watson Developer Cloud and Beautiful Soup Packages", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Requirement already up-to-date: watson-developer-cloud==1.0.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s09d-b77f39f57cb829-623e8809de07/.local/lib/python3.5/site-packages\nRequirement already up-to-date: python-dateutil>=2.5.3 in /usr/local/src/conda3_runtime.v28/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from watson-developer-cloud==1.0.1)\nRequirement already up-to-date: pysolr<4.0,>=3.3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s09d-b77f39f57cb829-623e8809de07/.local/lib/python3.5/site-packages (from watson-developer-cloud==1.0.1)\nRequirement already up-to-date: requests<3.0,>=2.0 in /usr/local/src/conda3_runtime.v28/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from watson-developer-cloud==1.0.1)\nRequirement already up-to-date: pyOpenSSL>=16.2.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s09d-b77f39f57cb829-623e8809de07/.local/lib/python3.5/site-packages (from watson-developer-cloud==1.0.1)\nRequirement already up-to-date: six>=1.5 in /usr/local/src/conda3_runtime.v28/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from python-dateutil>=2.5.3->watson-developer-cloud==1.0.1)\nRequirement already up-to-date: chardet<3.1.0,>=3.0.2 in /usr/local/src/conda3_runtime.v28/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud==1.0.1)\nRequirement already up-to-date: idna<2.7,>=2.5 in /usr/local/src/conda3_runtime.v28/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud==1.0.1)\nRequirement already up-to-date: urllib3<1.23,>=1.21.1 in /usr/local/src/conda3_runtime.v28/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud==1.0.1)\nRequirement already up-to-date: certifi>=2017.4.17 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s09d-b77f39f57cb829-623e8809de07/.local/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud==1.0.1)\nRequirement already up-to-date: cryptography>=2.1.4 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s09d-b77f39f57cb829-623e8809de07/.local/lib/python3.5/site-packages (from pyOpenSSL>=16.2.0->watson-developer-cloud==1.0.1)\nRequirement already up-to-date: cffi>=1.7; platform_python_implementation != \"PyPy\" in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s09d-b77f39f57cb829-623e8809de07/.local/lib/python3.5/site-packages (from cryptography>=2.1.4->pyOpenSSL>=16.2.0->watson-developer-cloud==1.0.1)\nRequirement already up-to-date: asn1crypto>=0.21.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s09d-b77f39f57cb829-623e8809de07/.local/lib/python3.5/site-packages (from cryptography>=2.1.4->pyOpenSSL>=16.2.0->watson-developer-cloud==1.0.1)\nRequirement already up-to-date: pycparser in /usr/local/src/conda3_runtime.v28/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from cffi>=1.7; platform_python_implementation != \"PyPy\"->cryptography>=2.1.4->pyOpenSSL>=16.2.0->watson-developer-cloud==1.0.1)\n"
                }
            ], 
            "source": "!pip install --upgrade watson-developer-cloud==1.0.1"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Requirement already up-to-date: beautifulsoup4 in /usr/local/src/conda3_runtime.v28/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages\r\n"
                }
            ], 
            "source": "!pip install --upgrade beautifulsoup4"
        }, 
        {
            "source": "<a id=\"pixie\"></a>\n### 1.2 Install PixieDust Library\nThis notebook provides an overview of how to use the PixieDust library to analyze and visualize various data sets. If you are new to PixieDust or would like to learn more about the library, please go to this [Introductory Notebook](https://apsportal.ibm.com/exchange/public/entry/view/5b000ed5abda694232eb5be84c3dd7c1) or visit the [PixieDust Github](https://ibm-cds-labs.github.io/pixiedust/). The `Setup` section for this notebook uses instructions from the [Intro To PixieDust](https://github.com/ibm-cds-labs/pixiedust/blob/master/notebook/Intro%20to%20PixieDust.ipynb) notebook.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Ensure you are running the latest version of PixieDust by running the following cell. Do not run this cell if you installed PixieDust locally from source and want to continue to run PixieDust from source.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Requirement already up-to-date: pixiedust in /usr/local/src/bluemix_jupyter_bundle.v82/notebook/lib/extra\nRequirement already up-to-date: astunparse in /usr/local/src/conda3_runtime.v28/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pixiedust)\nRequirement already up-to-date: lxml in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s09d-b77f39f57cb829-623e8809de07/.local/lib/python3.5/site-packages (from pixiedust)\nRequirement already up-to-date: markdown in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s09d-b77f39f57cb829-623e8809de07/.local/lib/python3.5/site-packages (from pixiedust)\nRequirement already up-to-date: geojson in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s09d-b77f39f57cb829-623e8809de07/.local/lib/python3.5/site-packages (from pixiedust)\nRequirement already up-to-date: mpld3 in /usr/local/src/conda3_runtime.v28/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pixiedust)\nRequirement already up-to-date: wheel<1.0,>=0.23.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s09d-b77f39f57cb829-623e8809de07/.local/lib/python3.5/site-packages (from astunparse->pixiedust)\nRequirement already up-to-date: six<2.0,>=1.6.1 in /usr/local/src/conda3_runtime.v28/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from astunparse->pixiedust)\n"
                }
            ], 
            "source": "!pip install --user --upgrade pixiedust"
        }, 
        {
            "source": "<a id=\"restart\"></a>\n### 1.3 Restart Kernel\n> Required after installs/upgrades only.\n\nIf any libraries were just installed or upgraded, <span style=\"color: red\">restart the kernel</span> before continuing. After this has been done once, you might want to comment out the `!pip install` lines above for cleaner output and a faster \"Run All\".", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"setup2\"></a>\n### 1.4 Import Packages and Libraries\n> Tip: To check if you have a package installed, open a new cell and write: `help(<package-name>)`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import json\nimport sys\n\nimport watson_developer_cloud\nfrom watson_developer_cloud import ToneAnalyzerV3, VisualRecognitionV3\nfrom watson_developer_cloud import NaturalLanguageUnderstandingV1\nfrom watson_developer_cloud.natural_language_understanding_v1 import Features, EntitiesOptions, KeywordsOptions, EmotionOptions, SentimentOptions\n\nimport operator\nfrom functools import reduce\nfrom io import StringIO\nimport numpy as np\nfrom bs4 import BeautifulSoup as bs\nfrom operator import itemgetter\nfrom os.path import join, dirname\nimport pandas as pd\nimport numpy as np\nimport requests\nimport pixiedust\n\n# Suppress some pandas warnings\npd.options.mode.chained_assignment = None  # default='warn'"
        }, 
        {
            "source": "<a id='setup3'></a>\n### 1.5 Add Service Credentials From IBM Cloud for Watson Services\nEdit the following cell to provide your credentials for Watson Visual Recognition, Natural Language Understanding and Tone Analyzer.\n\n###  <span style=\"color: red\"> _User Input_</span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 75, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 76, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create the Watson clients\n\nnlu = watson_developer_cloud.NaturalLanguageUnderstandingV1(version='2017-02-27',\n                                                            username=NATURAL_LANGUAGE_UNDERSTANDING_USERNAME,\n                                                            password=NATURAL_LANGUAGE_UNDERSTANDING_PASSWORD)\ntone_analyzer = ToneAnalyzerV3(version='2016-05-19',\n                               username=TONE_ANALYZER_USERNAME,\n                               password=TONE_ANALYZER_PASSWORD)\n\nvisual_recognition = VisualRecognitionV3('2016-05-20', api_key=VISUAL_RECOGNITION_API_KEY)"
        }, 
        {
            "source": "<a id='load'></a> \n## 2. Load Data\n\n### 2.1 Load data from Object Storage\n\n**Select the cell below and place your cursor on an empty line below the comment.** Load the CSV file you want to enrich by clicking on the 10/01 icon (upper right), then click `Insert to code` under the file you want to enrich, and choose `Insert Pandas DataFrame`.\n\n###  <span style=\"color: red\"> _User Input_</span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 77, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 77, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Post ID</th>\n      <th>Permalink</th>\n      <th>Post Message</th>\n      <th>Type</th>\n      <th>Countries</th>\n      <th>Languages</th>\n      <th>Posted</th>\n      <th>Audience Targeting</th>\n      <th>Lifetime Post Total Reach</th>\n      <th>Lifetime Post organic reach</th>\n      <th>...</th>\n      <th>Lifetime Post consumers by type - photo view</th>\n      <th>Lifetime Post Consumptions by type - link clicks</th>\n      <th>Lifetime Post Consumptions by type - other clicks</th>\n      <th>Lifetime Post Consumptions by type - photo view</th>\n      <th>Lifetime Negative feedback - hide_all_clicks</th>\n      <th>Lifetime Negative feedback - hide_clicks</th>\n      <th>Lifetime Negative feedback - unlike_page_clicks</th>\n      <th>Lifetime Negative Feedback from Users by Type - hide_all_clicks</th>\n      <th>Lifetime Negative Feedback from Users by Type - hide_clicks</th>\n      <th>Lifetime Negative Feedback from Users by Type - unlike_page_clicks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Lifetime: The total number of people your Page...</td>\n      <td>Lifetime: The number of people who saw your Pa...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>Lifetime: The number of clicks anywhere in you...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Lifetime: The number of people who have given ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Lifetime: The number of times people have give...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>187446750783_10153359024455784</td>\n      <td>https://www.facebook.com/ibmwatson/posts/10153...</td>\n      <td>Cheers to a wonderful New Year with Chef Watso...</td>\n      <td>Photo</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12/31/15 6:28</td>\n      <td></td>\n      <td>2291</td>\n      <td>2291</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>21</td>\n      <td>27.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>187446750783_10153215851080784</td>\n      <td>https://www.facebook.com/ibmwatson/posts/10153...</td>\n      <td>IBM Watson's cover photo</td>\n      <td>Photo</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12/31/15 6:26</td>\n      <td></td>\n      <td>158</td>\n      <td>158</td>\n      <td>...</td>\n      <td>307.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>544.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>187446750783_10153357233820784</td>\n      <td>https://www.facebook.com/ibmwatson/posts/10153...</td>\n      <td>What is Watson? IBM Watson is a technology pla...</td>\n      <td>Photo</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12/30/15 7:00</td>\n      <td></td>\n      <td>4203</td>\n      <td>4203</td>\n      <td>...</td>\n      <td>67.0</td>\n      <td>26</td>\n      <td>102.0</td>\n      <td>94.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>187446750783_10153355476175784</td>\n      <td>https://www.facebook.com/ibmwatson/posts/10153...</td>\n      <td>Your new personal shopping assistant, with the...</td>\n      <td>Link</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>12/29/15 6:26</td>\n      <td></td>\n      <td>3996</td>\n      <td>3996</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>44</td>\n      <td>20.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 52 columns</p>\n</div>", 
                        "text/plain": "                          Post ID  \\\n0                             NaN   \n1  187446750783_10153359024455784   \n2  187446750783_10153215851080784   \n3  187446750783_10153357233820784   \n4  187446750783_10153355476175784   \n\n                                           Permalink  \\\n0                                                NaN   \n1  https://www.facebook.com/ibmwatson/posts/10153...   \n2  https://www.facebook.com/ibmwatson/posts/10153...   \n3  https://www.facebook.com/ibmwatson/posts/10153...   \n4  https://www.facebook.com/ibmwatson/posts/10153...   \n\n                                        Post Message   Type  Countries  \\\n0                                                NaN    NaN        NaN   \n1  Cheers to a wonderful New Year with Chef Watso...  Photo        NaN   \n2                           IBM Watson's cover photo  Photo        NaN   \n3  What is Watson? IBM Watson is a technology pla...  Photo        NaN   \n4  Your new personal shopping assistant, with the...   Link        NaN   \n\n   Languages         Posted Audience Targeting  \\\n0        NaN            NaN                NaN   \n1        NaN  12/31/15 6:28                      \n2        NaN  12/31/15 6:26                      \n3        NaN  12/30/15 7:00                      \n4        NaN  12/29/15 6:26                      \n\n                           Lifetime Post Total Reach  \\\n0  Lifetime: The total number of people your Page...   \n1                                               2291   \n2                                                158   \n3                                               4203   \n4                                               3996   \n\n                         Lifetime Post organic reach  \\\n0  Lifetime: The number of people who saw your Pa...   \n1                                               2291   \n2                                                158   \n3                                               4203   \n4                                               3996   \n\n                                 ...                                  \\\n0                                ...                                   \n1                                ...                                   \n2                                ...                                   \n3                                ...                                   \n4                                ...                                   \n\n  Lifetime Post consumers by type - photo view  \\\n0                                          NaN   \n1                                          4.0   \n2                                        307.0   \n3                                         67.0   \n4                                          NaN   \n\n    Lifetime Post Consumptions by type - link clicks  \\\n0  Lifetime: The number of clicks anywhere in you...   \n1                                                 21   \n2                                                NaN   \n3                                                 26   \n4                                                 44   \n\n  Lifetime Post Consumptions by type - other clicks  \\\n0                                               NaN   \n1                                              27.0   \n2                                               NaN   \n3                                             102.0   \n4                                              20.0   \n\n  Lifetime Post Consumptions by type - photo view  \\\n0                                             NaN   \n1                                             4.0   \n2                                           544.0   \n3                                            94.0   \n4                                             NaN   \n\n        Lifetime Negative feedback - hide_all_clicks  \\\n0  Lifetime: The number of people who have given ...   \n1                                                NaN   \n2                                                NaN   \n3                                                NaN   \n4                                                NaN   \n\n  Lifetime Negative feedback - hide_clicks  \\\n0                                      NaN   \n1                                      NaN   \n2                                      NaN   \n3                                      NaN   \n4                                      NaN   \n\n  Lifetime Negative feedback - unlike_page_clicks  \\\n0                                             NaN   \n1                                             NaN   \n2                                             NaN   \n3                                             NaN   \n4                                             NaN   \n\n  Lifetime Negative Feedback from Users by Type - hide_all_clicks  \\\n0  Lifetime: The number of times people have give...                \n1                                                NaN                \n2                                                NaN                \n3                                                NaN                \n4                                                NaN                \n\n  Lifetime Negative Feedback from Users by Type - hide_clicks  \\\n0                                                NaN            \n1                                                NaN            \n2                                                NaN            \n3                                                NaN            \n4                                                NaN            \n\n  Lifetime Negative Feedback from Users by Type - unlike_page_clicks  \n0                                                NaN                  \n1                                                NaN                  \n2                                                NaN                  \n3                                                NaN                  \n4                                                NaN                  \n\n[5 rows x 52 columns]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 78, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Make sure this uses the variable above. The number will vary in the inserted code.\ntry:\n    df = df_data_2\nexcept NameError as e:\n    print('Error: Setup is incorrect or incomplete.\\n')\n    print('Follow the instructions to insert the pandas DataFrame above, and edit to')\n    print('make the generated df_data_# variable match the variable used here.')\n    raise"
        }, 
        {
            "source": "**Select the cell below and place your cursor on an empty line below the comment.** \nPut in the credentials for the file you want to enrich by clicking on the 10/01 (upper right), then click `Insert to code` under the file you want to enrich, and choose `Insert Credentials`.\n\n**Change the inserted variable name to `credentials_1`**\n\n###  <span style=\"color: red\"> _User Input_</span> ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 79, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 80, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Make sure this uses the variable above. The number will vary in the inserted code.\ntry:\n    credentials = credentials_2\nexcept NameError as e:\n    print('Error: Setup is incorrect or incomplete.\\n')\n    print('Follow the instructions to insert the file credentials above, and edit to')\n    print('make the generated credentials_# variable match the variable used here.')\n    raise"
        }, 
        {
            "source": "<a id='prepare'></a>\n## 3. Prepare Data\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id='prepare1'></a>\n###  3.1 Data Cleansing with Python\nRenaming columns, removing noticeable noise in the data, pulling out URLs and appending to a new column to run through NLU.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 81, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.rename(columns={'Post Message': 'Text'}, inplace=True)"
        }, 
        {
            "execution_count": 82, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Drop the rows that have NaN for the text.\ndf.dropna(subset=['Text'], inplace=True)"
        }, 
        {
            "execution_count": 83, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_http = df[\"Text\"].str.partition(\"http\")\ndf_www = df[\"Text\"].str.partition(\"www\")\n\n# Combine delimiters with actual links\ndf_http[\"Link\"] = df_http[1].map(str) + df_http[2]\ndf_www[\"Link1\"] = df_www[1].map(str) + df_www[2]\n\n# Include only Link columns\ndf_http.drop(df_http.columns[0:3], axis=1, inplace = True)\ndf_www.drop(df_www.columns[0:3], axis=1, inplace = True)\n\n# Merge http and www DataFrames\ndfmerge = pd.concat([df_http, df_www], axis=1)\n\n# The following steps will allow you to merge data columns from the left to the right\ndfmerge = dfmerge.apply(lambda x: x.str.strip()).replace('', np.nan)\n\n# Use fillna to fill any blanks with the Link1 column\ndfmerge[\"Link\"].fillna(dfmerge[\"Link1\"], inplace = True)\n\n# Delete Link1 (www column)\ndfmerge.drop(\"Link1\", axis=1, inplace = True)\n\n# Combine Link data frame\ndf = pd.concat([dfmerge,df], axis = 1)\n\n# Make sure text column is a string\ndf[\"Text\"] = df[\"Text\"].astype(\"str\")\n\n# Strip links from Text column\ndf['Text'] = df['Text'].apply(lambda x: x.split('http')[0])\ndf['Text'] = df['Text'].apply(lambda x: x.split('www')[0])"
        }, 
        {
            "source": "Pull thumbnail descriptions and image URLs using requests and beautiful soup.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 85, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Skipping url http://ibm.co/1VdSQDU: HTTPSConnectionPool(host='www.ibmchefwatson.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)\",),))\nSkipping url http://ibm.co/1VdSQDU: HTTPSConnectionPool(host='www.ibmchefwatson.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)\",),))\n"
                }
            ], 
            "source": "# Change links from objects to strings\nfor link in df.Link:\n    df.Link.to_string()\n\npiclinks = []\ndescription = []\nfor url in df[\"Link\"]:\n    if pd.isnull(url):\n        piclinks.append(\"\")\n        description.append(\"\")\n        continue\n        \n    try:\n        page3 = requests.get(url)\n        if page3.status_code != requests.codes.ok:\n            piclinks.append(\"\")\n            description.append(\"\")\n            continue\n    except Exception as e:\n        print(\"Skipping url %s: %s\" % (url, e))\n        piclinks.append(\"\")\n        description.append(\"\")\n        continue\n        \n    soup3 = bs(page3.text,\"lxml\")\n    \n    pic = soup3.find('meta', property =\"og:image\")\n    if pic:\n        piclinks.append(pic[\"content\"])\n    else: \n        piclinks.append(\"\")\n    \n    content = None\n    desc = soup3.find(attrs={'name':'Description'})\n    if desc:\n        content = desc['content']\n    if not content or content == 'null':\n        # Try again with lowercase description\n        desc = soup3.find(attrs={'name':'description'})\n        if desc:\n            content = desc['content']\n    if not content or content == 'null':\n        description.append(\"\")\n    else:\n        description.append(content)\n            \n# Save thumbnail descriptions to df in a column titled 'Thumbnails'\ndf[\"Thumbnails\"] = description\n# Save image links to df in a column titled 'Image'\ndf[\"Image\"] = piclinks"
        }, 
        {
            "source": "Convert shortened links to full links.\nUse requests module to pull extended links. This is only necessary if the Facebook page uses different links than the articles themselves. For this example we are using IBM Watson's Facebook export which uses an IBM link. \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 88, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Skipping link http://ibm.co/1VdSQDU: HTTPSConnectionPool(host='www.ibmchefwatson.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)\",),))\nSkipping link http://ibm.co/1VdSQDU: HTTPSConnectionPool(host='www.ibmchefwatson.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')],)\",),))\n"
                }
            ], 
            "source": "shortlink = df[\"Link\"]\nextendedlink = []\n\nfor link in shortlink:\n    if isinstance(link, float):  # Float is not a URL, probably NaN.\n        extendedlink.append('')\n    else:\n        try:\n            extended_link = requests.Session().head(link, allow_redirects=True).url\n            extendedlink.append(extended_link)\n        except Exception as e:\n            print(\"Skipping link %s: %s\" % (link, e))\n            extendedlink.append('')\n\ndf[\"Extended Links\"] = extendedlink"
        }, 
        {
            "source": "<a id='enrich'></a> \n## 4. Enrichment Time!\n<a id='nlupost'></a>\n###  4.1 NLU for the Post Text\nBelow uses Natural Language Understanding to iterate through each post and extract the enrichment features we want to use in our future analysis.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 90, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Define the list of features to get enrichment values for entities, keywords, emotion and sentiment\nfeatures = Features(entities=EntitiesOptions(), keywords=KeywordsOptions(), emotion=EmotionOptions(), sentiment=SentimentOptions())\n\noverallSentimentScore = []\noverallSentimentType = []\nhighestEmotion = []\nhighestEmotionScore = []\nkywords = []\nentities = []\n\n# Go through every response and enrich the text using NLU.\nfor text in df['Text']:\n    # We are assuming English to avoid errors when the language cannot be detected.\n    enriched_json = nlu.analyze(text=text, features=features, language='en')\n\n    # Get the SENTIMENT score and type\n    if 'sentiment' in enriched_json:\n        if('score' in enriched_json['sentiment'][\"document\"]):\n            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n        else:\n            overallSentimentScore.append('0')\n\n        if('label' in enriched_json['sentiment'][\"document\"]):\n            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n        else:\n            overallSentimentType.append('0')\n    else:\n        overallSentimentScore.append('0')\n        overallSentimentType.append('0')\n\n    # Read the EMOTIONS into a dict and get the key (emotion) with maximum value\n    if 'emotion' in enriched_json:\n        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n        highestEmotion.append(me)\n        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n    else:\n        highestEmotion.append(\"\")\n        highestEmotionScore.append(\"\")\n\n    # Iterate and get KEYWORDS with a confidence of over 70%\n    if 'keywords' in enriched_json:\n        tmpkw = []\n        for kw in enriched_json['keywords']:\n            if(float(kw[\"relevance\"]) >= 0.7):\n                tmpkw.append(kw[\"text\"])\n        # Convert multiple keywords in a list to a string and append the string\n        kywords.append(', '.join(tmpkw))\n    else:\n        kywords.append(\"\")\n            \n    # Iterate and get Entities with a confidence of over 30%\n    if 'entities' in enriched_json:\n        tmpent = []\n        for ent in enriched_json['entities']: \n            if(float(ent[\"relevance\"]) >= 0.3):\n                tmpent.append(ent[\"type\"])\n \n        # Convert multiple entities in a list to a string and append the string\n        entities.append(', '.join(tmpent))\n    else:\n        entities.append(\"\")    \n    \n# Create columns from the list and append to the DataFrame\nif highestEmotion:\n    df['TextHighestEmotion'] = highestEmotion\nif highestEmotionScore:\n    df['TextHighestEmotionScore'] = highestEmotionScore\n\nif overallSentimentType:\n    df['TextOverallSentimentType'] = overallSentimentType\nif overallSentimentScore:\n    df['TextOverallSentimentScore'] = overallSentimentScore\n\ndf['TextKeywords'] = kywords\ndf['TextEntities'] = entities"
        }, 
        {
            "source": "After we extract all of the Keywords and Entities from each Post, we have columns with multiple Keywords and Entities separated by commas. For our Analysis in Part II, we also wanted the top Keyword and Entity for each Post. Because of this, we added two new columns to capture the `MaxTextKeyword` and `MaxTextEntity`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 91, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Choose first of Keywords,Concepts, Entities\ndf[\"MaxTextKeywords\"] = df[\"TextKeywords\"].apply(lambda x: x.split(',')[0])\ndf[\"MaxTextEntity\"] = df[\"TextEntities\"].apply(lambda x: x.split(',')[0])"
        }, 
        {
            "source": "<a id='nlutn'></a>\n###  4.2 NLU for Thumbnail Text\n\nWe will repeat the same process for Thumbnails and Article Text.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 92, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Define the list of features to get enrichment values for entities, keywords, emotion and sentiment\nfeatures = Features(entities=EntitiesOptions(), keywords=KeywordsOptions(), emotion=EmotionOptions(), sentiment=SentimentOptions())\n\noverallSentimentScore = []\noverallSentimentType = []\nhighestEmotion = []\nhighestEmotionScore = []\nkywords = []\nentities = []\n\n# Go through every response and enrich the text using NLU.\nfor text in df['Thumbnails']:\n    if not text:\n        overallSentimentScore.append(' ')\n        overallSentimentType.append(' ')\n        highestEmotion.append(' ')\n        highestEmotionScore.append(' ')\n        kywords.append(' ')\n        entities.append(' ')\n        continue\n\n    enriched_json = nlu.analyze(text=text, features=features, language='en')\n\n    # Get the SENTIMENT score and type\n    if 'sentiment' in enriched_json:\n        if('score' in enriched_json['sentiment'][\"document\"]):\n            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n        else:\n            overallSentimentScore.append(\"\")\n\n        if('label' in enriched_json['sentiment'][\"document\"]):\n            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n        else:\n            overallSentimentType.append(\"\")\n\n    # Read the EMOTIONS into a dict and get the key (emotion) with maximum value\n    if 'emotion' in enriched_json:\n        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n        highestEmotion.append(me)\n        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n\n    else:\n        highestEmotion.append(\"\")\n        highestEmotionScore.append(\"\")\n\n    # Iterate and get KEYWORDS with a confidence of over 70%\n    if 'keywords' in enriched_json:\n        tmpkw = []\n        for kw in enriched_json['keywords']:\n            if(float(kw[\"relevance\"]) >= 0.7):\n                tmpkw.append(kw[\"text\"])\n        # Convert multiple keywords in a list to a string and append the string\n        kywords.append(', '.join(tmpkw))\n     \n    # Iterate and get Entities with a confidence of over 30%\n    if 'entities' in enriched_json:\n        tmpent = []\n        for ent in enriched_json['entities']:              \n            if(float(ent[\"relevance\"]) >= 0.3):\n                tmpent.append(ent[\"type\"])\n        # Convert multiple entities in a list to a string and append the string\n        entities.append(', '.join(tmpent))\n    else:\n        entities.append(\"\")     \n  \n# Create columns from the list and append to the DataFrame\nif highestEmotion:\n    df['ThumbnailHighestEmotion'] = highestEmotion\nif highestEmotionScore:\n    df['ThumbnailHighestEmotionScore'] = highestEmotionScore\n\nif overallSentimentType:\n    df['ThumbnailOverallSentimentType'] = overallSentimentType\nif overallSentimentScore:\n    df['ThumbnailOverallSentimentScore'] = overallSentimentScore\n\ndf['ThumbnailKeywords'] = kywords\ndf['ThumbnailEntities'] = entities"
        }, 
        {
            "execution_count": 93, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Set 'Max' to first one from keywords and entities lists\ndf[\"MaxThumbnailKeywords\"] = df[\"ThumbnailKeywords\"].apply(lambda x: x.split(',')[0])\ndf[\"MaxThumbnailEntity\"] = df[\"ThumbnailEntities\"].apply(lambda x: x.split(',')[0])"
        }, 
        {
            "source": "<a id='nlulink'></a> \n### 4.3 NLU for Article Text", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 94, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Define the list of features to get enrichment values for entities, keywords, emotion and sentiment\nfeatures = Features(entities=EntitiesOptions(), keywords=KeywordsOptions(), emotion=EmotionOptions(), sentiment=SentimentOptions())\n\noverallSentimentScore = []\noverallSentimentType = []\nhighestEmotion = []\nhighestEmotionScore = []\nkywords = []\nentities = []\narticle_text = []\n        \n# Go through every response and enrich the article using NLU\nfor url in df['Extended Links']:\n    if not url:\n        overallSentimentScore.append(' ')\n        overallSentimentType.append(' ')\n        highestEmotion.append(' ')\n        highestEmotionScore.append(' ')\n        kywords.append(' ')\n        entities.append(' ')\n        article_text.append(' ')\n        continue\n\n    # Run links through NLU to get entities, keywords, emotion and sentiment.\n    # Use return_analyzed_text to extract text for Tone Analyzer to use.\n    enriched_json = nlu.analyze(url=url,\n                                features=features,\n                                language='en',\n                                return_analyzed_text=True)\n    \n    article_text.append(enriched_json[\"analyzed_text\"])\n\n    # Get the SENTIMENT score and type\n    if 'sentiment' in enriched_json:\n        if('score' in enriched_json['sentiment'][\"document\"]):\n            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n        else:\n            overallSentimentScore.append('None')\n\n        if('label' in enriched_json['sentiment'][\"document\"]):\n            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n        else:\n            overallSentimentType.append('')\n\n    # Read the EMOTIONS into a dict and get the key (emotion) with maximum value\n    if 'emotion' in enriched_json:\n        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n        highestEmotion.append(me)\n        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n\n    else:\n        highestEmotion.append('')\n        highestEmotionScore.append('')\n\n    # Iterate and get KEYWORDS with a confidence of over 70%\n    if 'keywords' in enriched_json:\n        tmpkw = []\n        for kw in enriched_json['keywords']:\n            if(float(kw[\"relevance\"]) >= 0.7):\n                tmpkw.append(kw[\"text\"])\n        # Convert multiple keywords in a list to a string and append the string\n        kywords.append(', '.join(tmpkw))\n    else: \n        kywords.append(\"\")\n            \n    # Iterate and get Entities with a confidence of over 30%\n    if 'entities' in enriched_json:\n        tmpent = []\n        for ent in enriched_json['entities']:               \n            if(float(ent[\"relevance\"]) >= 0.3):\n                tmpent.append(ent[\"type\"])\n        # Convert multiple entities in a list to a string and append the string\n        entities.append(', '.join(tmpent))\n    else:\n        entities.append(\"\")\n    \n# Create columns from the list and append to the DataFrame\nif highestEmotion:\n    df['LinkHighestEmotion'] = highestEmotion\nif highestEmotionScore:\n    df['LinkHighestEmotionScore'] = highestEmotionScore\n\nif overallSentimentType:\n    df['LinkOverallSentimentType'] = overallSentimentType\nif overallSentimentScore:\n    df['LinkOverallSentimentScore'] = overallSentimentScore\n\ndf['LinkKeywords'] = kywords\ndf['LinkEntities'] = entities\ndf['Article Text'] = article_text"
        }, 
        {
            "execution_count": 95, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Set 'Max' to first one from keywords and entities lists\ndf[\"MaxLinkKeywords\"] = df[\"LinkKeywords\"].apply(lambda x: x.split(',')[0])\ndf[\"MaxLinkEntity\"] = df[\"LinkEntities\"].apply(lambda x: x.split(',')[0])"
        }, 
        {
            "source": " <a id='tonepost'></a> \n### 4.4 Tone Analyzer for Post Text", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 96, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "highestEmotionTone = []\nemotionToneScore = []\n\nhighestLanguageTone = []\nlanguageToneScore = []\n\nhighestSocialTone = []\nsocialToneScore = []\n\nfor text in df['Text']:\n    enriched_json = tone_analyzer.tone(text, content_type='text/plain')\n        \n    me = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['tone_name']      \n    highestEmotionTone.append(me)\n    you = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['score']\n    emotionToneScore.append(you)\n            \n    me1 = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['tone_name']      \n    highestLanguageTone.append(me1)\n    you1 = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['score']\n    languageToneScore.append(you1)\n            \n    me2 = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['tone_name']      \n    highestSocialTone.append(me2)\n    you2 = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['score']\n    socialToneScore.append(you2)\n    \ndf['highestEmotionTone'] = highestEmotionTone    \ndf['emotionToneScore'] = emotionToneScore\ndf['languageToneScore'] = languageToneScore\ndf['highestLanguageTone'] = highestLanguageTone\ndf['highestSocialTone'] = highestSocialTone    \ndf['socialToneScore'] = socialToneScore"
        }, 
        {
            "source": "<a id='tonearticle'></a> \n### 4.5 Tone Analyzer for Article Text", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Unlike NLU, Tone Analyzer cannot iterate through a URL, so we used NLU (above) to pull the Article Text from the URL and append it to the DataFrame.\n\nNow using Tone Analyzer, we iterate through the `Article Text` column which contains all of the free form text from the articles contained in the Facebook posts. \n\nWe are using Tone Analyzer to gather the top Social, Language and Emotion Tones from the Articles.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 97, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "highestEmotionTone = []\nemotionToneScore = []\n\nhighestLanguageTone = []\nlanguageToneScore = []\n\nhighestSocialTone = []\nsocialToneScore = []\n\nfor text in df['Article Text']:\n    if not text:\n        emotionToneScore.append(' ')\n        highestEmotionTone.append(' ')     \n        languageToneScore.append(' ')\n        highestLanguageTone.append(' ')     \n        socialToneScore.append(' ')\n        highestSocialTone.append(' ')  \n        continue\n\n    enriched_json = tone_analyzer.tone(text, content_type='text/plain', sentences=False)\n        \n    maxTone = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['tone_name']      \n    highestEmotionTone.append(maxTone)\n    maxToneScore = max(enriched_json[\"document_tone\"][\"tone_categories\"][0][\"tones\"], key = itemgetter('score'))['score']\n    emotionToneScore.append(maxToneScore)\n            \n    maxLanguageTone = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['tone_name']      \n    highestLanguageTone.append(maxLanguageTone)\n    maxLanguageScore = max(enriched_json[\"document_tone\"][\"tone_categories\"][1][\"tones\"], key = itemgetter('score'))['score']\n    languageToneScore.append(maxLanguageScore)\n            \n    maxSocial = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['tone_name']      \n    highestSocialTone.append(maxSocial)\n    maxSocialScore = max(enriched_json[\"document_tone\"][\"tone_categories\"][2][\"tones\"], key = itemgetter('score'))['score']\n    socialToneScore.append(maxSocialScore)\n   \ndf['articlehighestEmotionTone'] = highestEmotionTone\ndf['articleEmotionToneScore'] = emotionToneScore\ndf['articlelanguageToneScore'] = languageToneScore\ndf['articlehighestLanguageTone'] = highestLanguageTone\ndf['articlehighestSocialTone'] = highestSocialTone\ndf['articlesocialToneScore'] = socialToneScore"
        }, 
        {
            "source": "<a id='visual'></a> \n### 4.6 Visual Recognition\nBelow uses Visual Recognition to classify the thumbnail images.\n\n> NOTE: When using the **free tier** of Visual Recognition, _classify_ has a limit of 250 images per day.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 98, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Skipping url http://www.ibm.com/watson/assets/img/cloud/hero_cloud.jpg: Error: Unknown error, Code: 400\nSkipping url https://fortunedotcom.files.wordpress.com/2015/11/gettyimages-509786385.jpg: Error: unspecified error - please try again\n, Code: 500\nSkipping url http://www-03.ibm.com/press/img/ibmpos_blu_feed.jpg: Error: Unknown error, Code: 400\nSkipping url http://www-03.ibm.com/press/img/ibmpos_blu_feed.jpg: Error: Unknown error, Code: 400\n"
                }
            ], 
            "source": "piclinks = df[\"Image\"]\n\npicclass = []\npiccolor = []\npictype1 = []\npictype2 = []\npictype3 = []\n\nfor pic in piclinks:\n    if not pic or pic == 'default-img':\n        picclass.append(' ')\n        piccolor.append(' ')\n        pictype1.append(' ')\n        pictype2.append(' ')\n        pictype3.append(' ')\n        continue\n\n    classes = []\n    enriched_json = {}\n    try:\n        enriched_json = visual_recognition.classify(parameters=json.dumps({'url': pic}))\n    except Exception as e:\n        print(\"Skipping url %s: %s\" % (pic, e))\n\n    if 'error' in enriched_json:\n        print(enriched_json['error'])\n    if 'images' in enriched_json and 'classifiers' in enriched_json['images'][0]:\n        classes = enriched_json['images'][0][\"classifiers\"][0][\"classes\"]\n\n    color1 = None\n    class1 = None\n    type_hierarchy1 = None\n\n    for iclass in classes:\n        # Grab the first color, first class, and first type hierarchy.\n        # Note: Usually you'd filter by 'score' too.\n        if not type_hierarchy1 and 'type_hierarchy' in iclass:\n            type_hierarchy1 = iclass['type_hierarchy']\n        if not class1:\n            class1 = iclass['class']\n        if not color1 and iclass['class'].endswith(' color'):\n            color1 = iclass['class'][:-len(' color')]\n        if type_hierarchy1 and class1 and color1:\n            # We are only using 1 of each per image. When we have all 3, break.\n            break\n\n    picclass.append(class1 or ' ')\n    piccolor.append(color1 or ' ')\n    type_split = (type_hierarchy1 or '/ / / ').split('/')\n    pictype1.append(type_split[1] if len(type_split) > 1 else '-')\n    pictype2.append(type_split[2] if len(type_split) > 2 else '- ')\n    pictype3.append(type_split[3] if len(type_split) > 3 else '-')\n\ndf[\"Image Color\"] = piccolor\ndf[\"Image Class\"] = picclass\ndf[\"Image Type\"] = pictype1\ndf[\"Image Subtype\"] = pictype2\ndf[\"Image Subtype2\"] = pictype3"
        }, 
        {
            "source": " <a id='write'></a>\n## Enrichment is now COMPLETE!\n<a id='write1'></a> \nSave a copy of the enriched DataFrame as a file in IBM Object Storage.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 99, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def put_file(credentials, local_file_name):\n    \"\"\"Put the file in IBM Object Storage.\"\"\"\n    f = open(local_file_name,'r',encoding=\"utf-8\")\n    my_data = f.read()\n    data_to_send = my_data.encode(\"utf-8\")\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']},\n            'password': credentials['password']}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', credentials['container'], '/', local_file_name])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.put(url=url2, headers=headers2, data = data_to_send )\n    resp2.raise_for_status()\n    print('%s saved to IBM Object Storage. Status code=%s.' % (localfilename, resp2.status_code))"
        }, 
        {
            "execution_count": 105, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "KeyError", 
                    "evalue": "'filename'", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-105-f9da7c93bcd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build the enriched file name from the original filename.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlocalfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'enriched_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Write a CSV file from the enriched pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mKeyError\u001b[0m: 'filename'"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "# Build the enriched file name from the original filename.\nlocalfilename = 'enriched_' + credentials['filename']\n\n# Write a CSV file from the enriched pandas DataFrame.\ndf.to_csv(localfilename, index=False)\n\n# Use the above put_file method with credentials to put the file in Object Storage.\nput_file(credentials, localfilename)"
        }, 
        {
            "source": "<a id=\"part2\"></a> \n# Part II - Data Preparation\n<a id='prepare'></a>\n## 1. Prepare Data\n <a id='visualizations'></a>\n### 1.1 Prepare Multiple DataFrames for Visualizations\nBefore we can create the separate tables for each Watson feature we need to organize and reformat the data. First, we need to determine which data points are tied to metrics. Second, we need to make sure make sure each metric is numeric. _(This is necessary for PixieDust in Part III)_", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 106, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Determine which data points are tied to metrics and put them in a list\nmetrics = [\"Lifetime Post Total Reach\", \"Lifetime Post organic reach\", \"Lifetime Post Paid Reach\", \"Lifetime Post Total Impressions\", \"Lifetime Post Organic Impressions\", \n           \"Lifetime Post Paid Impressions\", \"Lifetime Engaged Users\", \"Lifetime Post Consumers\", \"Lifetime Post Consumptions\", \"Lifetime Negative feedback\", \"Lifetime Negative Feedback from Users\", \n           \"Lifetime Post Impressions by people who have liked your Page\", \"Lifetime Post reach by people who like your Page\", \"Lifetime Post Paid Impressions by people who have liked your Page\", \n           \"Lifetime Paid reach of a post by people who like your Page\", \"Lifetime People who have liked your Page and engaged with your post\", \"Lifetime Talking About This (Post) by action type - comment\", \n           \"Lifetime Talking About This (Post) by action type - like\", \"Lifetime Talking About This (Post) by action type - share\", \"Lifetime Post Stories by action type - comment\", \"Lifetime Post Stories by action type - like\", \n           \"Lifetime Post Stories by action type - share\", \"Lifetime Post consumers by type - link clicks\", \"Lifetime Post consumers by type - other clicks\", \"Lifetime Post consumers by type - photo view\", \"Lifetime Post Consumptions by type - link clicks\", \n           \"Lifetime Post Consumptions by type - other clicks\", \"Lifetime Post Consumptions by type - photo view\", \"Lifetime Negative feedback - hide_all_clicks\", \"Lifetime Negative feedback - hide_clicks\", \n           \"Lifetime Negative Feedback from Users by Type - hide_all_clicks\", \"Lifetime Negative Feedback from Users by Type - hide_clicks\"]\n"
        }, 
        {
            "source": "<a id='tone'></a> \n### 1.2 Create a Consolidated Tone DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Post Tone DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 107, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create a list with only Post Tone Values\npost_tones = [\"Text\",\"highestEmotionTone\", \"emotionToneScore\", \"languageToneScore\", \"highestLanguageTone\", \"highestSocialTone\", \"socialToneScore\"]\n\n# Append DataFrame with these metrics\npost_tones.extend(metrics)\n\n# Create a new DataFrame with tones and metrics\ndf_post_tones = df[post_tones]\n\n# Determine which tone values are suppose to be numeric and ensure they are numeric. \npost_numeric_values = [\"emotionToneScore\", \"languageToneScore\", \"socialToneScore\"]\nfor i in post_numeric_values:\n    df_post_tones[i] = pd.to_numeric(df_post_tones[i], errors='coerce')\n\n# Make all metrics numeric\nfor i in metrics:\n    df_post_tones[i] = pd.to_numeric(df_post_tones[i], errors='coerce')\n    \n# Drop NA Values in Tone Enrichment Columns\ndf_post_tones.dropna(subset=[\"socialToneScore\"] , inplace = True)\n\n\n# Add in a column to distinguish what portion the enrichment was happening \ndf_post_tones[\"Type\"] = \"Post\""
        }, 
        {
            "source": "#### Article Tone DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 108, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create a list with only Article Tone Values\narticle_tones = [\"Text\", \"articlehighestEmotionTone\", \"articleEmotionToneScore\", \"articlelanguageToneScore\", \"articlehighestLanguageTone\", \"articlehighestSocialTone\", \"articlesocialToneScore\"]\n\n# Append DataFrame with these metrics\narticle_tones.extend(metrics)\n\n# Create a new DataFrame with tones and metrics\ndf_article_tones = df[article_tones]\n\n# Determine which values are suppose to be numeric and ensure they are numeric. \nart_numeric_values = [\"articleEmotionToneScore\", \"articlelanguageToneScore\", \"articlesocialToneScore\"]\nfor i in art_numeric_values:\n    df_article_tones[i] = pd.to_numeric(df_article_tones[i], errors='coerce')\n    \n# Make all metrics numeric\nfor i in metrics:\n    df_article_tones[i] = pd.to_numeric(df_article_tones[i], errors='coerce')\n    \n# Drop NA Values in Tone Enrichment Columns\ndf_article_tones.dropna(subset=[\"articlesocialToneScore\"] , inplace = True)\n\n# Add in a column to distinguish what portion the enrichment was happening \ndf_article_tones[\"Type\"] = \"Article\""
        }, 
        {
            "source": "#### Combine Post and Article DataFrames to Make One Tone DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 109, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# First make the Column Headers the same\ndf_post_tones.rename(columns={\"highestEmotionTone\":\"Emotion Tone\", \"emotionToneScore\":\"Emotion Tone Score\", \"languageToneScore\": \"Language Tone Score\", \"highestLanguageTone\": \"Language Tone\", \"highestSocialTone\": \"Social Tone\", \"socialToneScore\":\"Social Tone Score\"\n}, inplace=True)\n\ndf_article_tones.rename(columns={\"articlehighestEmotionTone\":\"Emotion Tone\", \"articleEmotionToneScore\":\"Emotion Tone Score\", \"articlelanguageToneScore\": \"Language Tone Score\", \"articlehighestLanguageTone\": \"Language Tone\", \"articlehighestSocialTone\": \"Social Tone\", \"articlesocialToneScore\":\"Social Tone Score\"\n}, inplace=True)\n\n# Combine into one data frame\ndf_tones = pd.concat([df_post_tones, df_article_tones])"
        }, 
        {
            "source": " <a id='keyword'></a> \n### 1.3 Create a Consolidated Keyword DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": " #### Article Keyword DataFrame ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 110, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create a list with only Article Keywords\narticle_keywords = [\"Text\", \"MaxLinkKeywords\"]\n\n# Append DataFrame with these metrics\narticle_keywords.extend(metrics)\n\n# Create a new DataFrame with keywords and metrics\ndf_article_keywords = df[article_keywords]\n\n# Make all metrics numeric\nfor i in metrics:\n    df_article_keywords[i] = pd.to_numeric(df_article_keywords[i], errors='coerce')\n  \n# Drop NA Values in Keywords Column\n\ndf_article_keywords['MaxLinkKeywords'].replace(' ', np.nan, inplace=True)\ndf_article_keywords.dropna(subset=['MaxLinkKeywords'], inplace=True)\n\n# Add in a column to distinguish what portion the enrichment was happening \ndf_article_keywords[\"Type\"] = \"Article\""
        }, 
        {
            "source": "#### Thumbnail Keyword DataFrame ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 111, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create a list with only Thumbnail Keywords\nthumbnail_keywords = [\"Text\", \"MaxThumbnailKeywords\"]\n\n# Append DataFrame with these metrics\nthumbnail_keywords.extend(metrics)\n\n# Create a new DataFrame with keywords and metrics\ndf_thumbnail_keywords = df[thumbnail_keywords]\n\n\n# Make all metrics numeric\nfor i in metrics:\n    df_thumbnail_keywords[i] = pd.to_numeric(df_thumbnail_keywords[i], errors='coerce')\n    \n# Drop NA Values in Keywords Column\ndf_thumbnail_keywords['MaxThumbnailKeywords'].replace(' ', np.nan, inplace=True)\ndf_thumbnail_keywords.dropna(subset=['MaxThumbnailKeywords'], inplace=True)\n\n# Add in a column to distinguish what portion the enrichment was happening \ndf_thumbnail_keywords[\"Type\"] = \"Thumbnails\""
        }, 
        {
            "source": "#### Post Keyword DataFrame ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 112, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create a list with only Thumbnail Keywords\npost_keywords = [\"Text\", \"MaxTextKeywords\"]\n\n# Append DataFrame with these metrics\npost_keywords.extend(metrics)\n\n# Create a new DataFrame with keywords and metrics\ndf_post_keywords = df[post_keywords]\n\n# Make all metrics numeric\nfor i in metrics:\n    df_post_keywords[i] = pd.to_numeric(df_post_keywords[i], errors='coerce')\n    \n# Drop NA Values in Keywords Column\n\ndf_post_keywords['MaxTextKeywords'].replace(' ', np.nan, inplace=True)\ndf_post_keywords.dropna(subset=['MaxTextKeywords'], inplace=True)\n\n# Add in a column to distinguish what portion the enrichment was happening \ndf_post_keywords[\"Type\"] = \"Posts\""
        }, 
        {
            "source": "#### Combine Post, Thumbnail, and Article DataFrames to Make One Keywords DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 113, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# First make the column headers the same\ndf_post_keywords.rename(columns={\"MaxTextKeywords\": \"Keywords\"}, inplace=True)\n\ndf_thumbnail_keywords.rename(columns={\"MaxThumbnailKeywords\":\"Keywords\"}, inplace=True)\n\ndf_article_keywords.rename(columns={\"MaxLinkKeywords\":\"Keywords\"}, inplace=True)\n\n# Combine into one data frame\ndf_keywords = pd.concat([df_post_keywords, df_thumbnail_keywords, df_article_keywords])\n\n# Discard keywords with lower consumption to make charting easier\ndf_keywords = df_keywords[df_keywords[\"Lifetime Post Consumptions\"] > 175]"
        }, 
        {
            "source": "<a id='entity'></a>\n###  1.4 Create a Consolidated Entity DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Article Entity DataFrame ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 114, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create a list with only Article Keywords\narticle_entities = [\"Text\", \"MaxLinkEntity\"]\n\n# Append DataFrame with these metrics\narticle_entities.extend(metrics)\n\n# Create a new DataFrame with keywords and metrics\ndf_article_entities = df[article_entities]\n    \n# Make all metrics numeric\nfor i in metrics:\n    df_article_entities[i] = pd.to_numeric(df_article_entities[i], errors='coerce')\n    \n# Drop NA Values in Keywords Column\ndf_article_entities['MaxLinkEntity'] = df[\"MaxLinkEntity\"].replace(r'\\s+', np.nan, regex=True)\ndf_article_entities.dropna(subset=['MaxLinkEntity'], inplace=True)\n\n# Add in a column to distinguish what portion the enrichment was happening \ndf_article_entities[\"Type\"] = \"Article\""
        }, 
        {
            "source": "#### Thumbnail Entity DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 115, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create a list with only Thumbnail Keywords\nthumbnail_entities = [\"Text\", \"MaxThumbnailEntity\"]\n\n# Append DataFrame with these metrics\nthumbnail_entities.extend(metrics)\n\n# Create a new DataFrame with keywords and metrics\ndf_thumbnail_entities = df[thumbnail_entities]\n\n# Make all metrics numeric\nfor i in metrics:\n    df_thumbnail_entities[i] = pd.to_numeric(df_thumbnail_entities[i], errors='coerce')\n    \n# Drop NA Values in Keywords Column\ndf_thumbnail_entities['MaxThumbnailEntity'] = df_thumbnail_entities['MaxThumbnailEntity'].replace(r'\\s+', np.nan, regex=True)\ndf_thumbnail_entities.dropna(subset=['MaxThumbnailEntity'], inplace=True)\n\n# Add in a column to distinguish what portion the enrichment was happening \ndf_thumbnail_entities[\"Type\"] = \"Thumbnails\""
        }, 
        {
            "source": "#### Post Entity DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 116, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create a list with only Thumbnail Keywords\npost_entities = [\"Text\", \"MaxTextEntity\"]\n\n# Append DataFrame with these metrics\npost_entities.extend(metrics)\n\n# Create a new DataFrame with keywords and metrics\ndf_post_entities = df[post_entities]\n\n# Make all metrics numeric\nfor i in metrics:\n    df_post_entities[i] = pd.to_numeric(df_post_entities[i], errors='coerce')\n    \n# Drop NA Values in Keywords Column\ndf_post_entities['MaxTextEntity'] = df_post_entities['MaxTextEntity'].replace(r'\\s+', np.nan, regex=True)\ndf_post_entities.dropna(subset=['MaxTextEntity'], inplace=True)\n\n# Add in a column to distinguish what portion the enrichment was happening \ndf_post_entities[\"Type\"] = \"Posts\""
        }, 
        {
            "source": "#### Combine Post, Thumbnail, and Article DataFrames to Make One Entity DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 117, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# First make the column headers the same\ndf_post_entities.rename(columns={\"MaxTextEntity\": \"Entities\"}, inplace=True)\n\ndf_thumbnail_entities.rename(columns={\"MaxThumbnailEntity\":\"Entities\"}, inplace=True)\n\ndf_article_entities.rename(columns={\"MaxLinkEntity\":\"Entities\"}, inplace=True)\n\n# Combine into one data frame\ndf_entities = pd.concat([df_post_entities, df_thumbnail_entities, df_article_entities])\n\ndf_entities[\"Entities\"] = df_entities[\"Entities\"].replace('', np.nan)\ndf_entities.dropna(subset=[\"Entities\"], inplace=True)"
        }, 
        {
            "source": "<a id='image_dataframe'></a>\n###  1.5 Create a Consolidated Image DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Combine Metrics with Type Hierarchy, Class and Color to Make One Image DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 118, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Create a list with only Visual Recognition columns\npic_keywords = ['Image Type', 'Image Subtype', 'Image Subtype2', 'Image Class', 'Image Color']\n\n# Append DataFrame with these metrics\npic_keywords.extend(metrics)\n\n# Create a new DataFrame with keywords and metrics\ndf_pic_keywords = df[pic_keywords]\n\n# Make all metrics numeric\nfor i in metrics:\n    df_pic_keywords[i] = pd.to_numeric(df_pic_keywords[i], errors='coerce')"
        }, 
        {
            "source": "<a id=\"part3\"></a> \n# Part III\n<a id='2setup'></a> \n## 1. Setup\n<a id='2setup2'></a>\n###  1.1 Assign Variables\nAssign new DataFrames to variables. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 119, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "entities = df_entities\ntones = df_tones\nkeywords = df_keywords"
        }, 
        {
            "source": "<a id='2visual'></a>\n##  2. Visualize Data\n<a id='2visual1'></a> \n### 2.1 Run PixieDust Visualization Library with Display() API\nPixieDust lets you visualize your data in just a few clicks using the display() API. You can find more info at https://ibm-cds-labs.github.io/pixiedust/displayapi.html.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### We can use a pie chart to identify how post consumption was broken up by tone. \n\nClick on the `Options` button to change the chart.  Here are some things to try:\n* Add *Type* to make the breakdown show *Post* or *Article*.\n* Show *Social Tone* intead of *Emotion Tone* (or both).\n* Try a different metric.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 121, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "title": "Tone in Posts and Articles", 
                        "chartsize": "70", 
                        "mpld3": "false", 
                        "aggregation": "SUM", 
                        "rowCount": "100", 
                        "handlerId": "mapView", 
                        "valueFields": "Lifetime Post Consumptions", 
                        "keyFields": "Emotion Tone", 
                        "legend": "false"
                    }
                }
            }, 
            "outputs": [], 
            "source": "display(tones)"
        }, 
        {
            "source": "#### Now let's look at the same statistics as a bar chart.\n\nIt is the same line of code. Use the `Edit Metadata` button to see how PixieDust knows to show us a bar chart. If you don't have a button use the menu and select `View > Cell Toolbar > Edit Metadata`.\n\nA bar chart is better at showing more information. We added `Cluster By: Type` so we already see numbers for posts and articles. Notice what the chart tells you. *Joy* seems to be the dominant emotion. Click on `Options` and try this:\n\n* Change the aggregation to `AVG`.\n\nWhat emotion leads to higher average consumption?\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "title": "Tone in Posts and Articles", 
                        "chartsize": "70", 
                        "aggregation": "SUM", 
                        "clusterby": "Type", 
                        "rowCount": "100", 
                        "handlerId": "barChart", 
                        "valueFields": "Lifetime Post Consumptions", 
                        "rendererId": "matplotlib", 
                        "charttype": "grouped", 
                        "keyFields": "Emotion Tone", 
                        "legend": "true"
                    }
                }, 
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "display(tones)"
        }, 
        {
            "source": "#### Now let's look at the entities that were detected by Natural Language Understanding.\n\nThe following bar chart shows the entities that were detected. This time we are stacking negative feedback and \"likes\" to get a picture of the kind of feedback the entities were getting. We chose a horizontal, stacked bar chart with descending values for a little variety.\n\n* Try a different renderer and see what you get.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "orientation": "horizontal", 
                        "title": "Entities in Posts and Articles", 
                        "chartsize": "70", 
                        "aggregation": "SUM", 
                        "rowCount": "100", 
                        "handlerId": "barChart", 
                        "valueFields": "Lifetime Post Stories by action type - like,Lifetime Negative Feedback from Users", 
                        "rendererId": "matplotlib", 
                        "sortby": "Values ASC", 
                        "charttype": "stacked", 
                        "keyFields": "Entities"
                    }
                }, 
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "display(entities)"
        }, 
        {
            "source": "#### Next we look at the keywords detected by Natural Language Understanding\n\nThis time we are using the bokeh renderer for a different look. The renderers have different capabilities.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "orientation": "horizontal", 
                        "title": "Keywords in Posts, Articles and Thumbnails", 
                        "chartsize": "99", 
                        "mpld3": "false", 
                        "aggregation": "AVG", 
                        "clusterby": "Type", 
                        "rowCount": "100", 
                        "handlerId": "barChart", 
                        "valueFields": "Lifetime Post Consumptions", 
                        "rendererId": "bokeh", 
                        "sortby": "Values ASC", 
                        "timeseries": "false", 
                        "charttype": "stacked", 
                        "keyFields": "Keywords", 
                        "legend": "true"
                    }
                }, 
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "display(keywords)"
        }, 
        {
            "source": "#### Now let's take a look at what Visual Recognition can show us.\n\nSee how the images influenced the metrics. We've used visual recognition to identify a class and a type hierarchy for each image. We've also captured the top recognized color for each image. Our sample data doesn't have a significant number of data points, but these three charts demonstrate how you could:\n\n1. Recognize image classes that correlate to higher average post consumption.\n1. Add a type hierarchy for a higher level abstraction or to add grouping/stacking to the class data.\n1. Determine if image color correlates to user reactions.\n\nVisual recognition makes it surprisingly easy to do all of the above. Of course, you can easily try different metrics as you experiment. If you are not convinced that you should add ultramarine robot pictures to all of your articles, then you might want to do some research with a better data sample.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "title": "Image Class", 
                        "chartsize": "99", 
                        "aggregation": "AVG", 
                        "rowCount": "100", 
                        "handlerId": "barChart", 
                        "valueFields": "Lifetime Post Consumptions", 
                        "sortby": "Values ASC", 
                        "charttype": "stacked", 
                        "keyFields": "Image Class", 
                        "legend": "false"
                    }
                }
            }, 
            "outputs": [], 
            "source": "display(df_pic_keywords)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "title": "Image Type Hierarchy", 
                        "chartsize": "99", 
                        "aggregation": "AVG", 
                        "rowCount": "100", 
                        "handlerId": "barChart", 
                        "valueFields": "Lifetime Post Consumptions", 
                        "sortby": "Values ASC", 
                        "charttype": "stacked", 
                        "keyFields": "Image Type,Image Subtype,Image Subtype2", 
                        "legend": "false"
                    }
                }
            }, 
            "outputs": [], 
            "source": "display(df_pic_keywords)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "title": "Image Color", 
                        "chartsize": "99", 
                        "aggregation": "AVG", 
                        "rowCount": "100", 
                        "handlerId": "barChart", 
                        "valueFields": "Lifetime Post Consumptions", 
                        "sortby": "Values ASC", 
                        "charttype": "stacked", 
                        "keyFields": "Image Color", 
                        "legend": "false"
                    }
                }
            }, 
            "outputs": [], 
            "source": "display(df_pic_keywords)"
        }, 
        {
            "source": "# More Info.\nFor more information about PixieDust, check out the following:\n* PixieDust Documentation: https://ibm-cds-labs.github.io/pixiedust/index.html\n* PixieDust GitHub Repo: https://github.com/ibm-cds-labs/pixiedust\n\nVisit the Watson Accelerators portal to see more live patterns in action:\n* Watson Accelerators: http://www.watsonaccelerators.com\u00a0", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.0", 
            "name": "python3-spark20", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "celltoolbar": "Edit Metadata"
    }, 
    "nbformat": 4
}